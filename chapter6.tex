\chapter{Experimental Results}\label{ch:6}
We implement our optimizations in Linux/KVM version 3.0, which has paravirtual extensions for Power. To measure performance of {\em unmodified} KVM and KVM with our optimizations, we disabled the paravirtual extensions. We perform our experiments on Freescale QorIQ P2020 platform, which is optimized for single-threaded performance per watt for networking and telecom applications. Our system has a 1.2GHz processor with 32KB L1 cache and 512 KB L2 cache. We use RAMdisk for our experiments to eliminate I/O overheads.

Our benchmarks are described in Table~\ref{tab:kvm_performance}. We use four macrobenchmarks, namely Linux boot, Echo spawn, Find and Lame. These benchmarks have also been used in a previous performance study\cite{kvm_ppc_exittimings}. The others are microbenchmarks from the widely-used {\tt lmbench} toolset\cite{lmbench} and {\tt unixbench} toolset \cite{unixbench}. Linux boot and Echo spawn execute a large number of privileged instructions, while Find executes relatively fewer privileged instructions. Lame has to do some IO and a lot of computation that does not need to exit to the hypervisor, thus representing an average compute-intensive workload. 

{\tt Lmbench} is a performance analysis suite of micro-benchmarks for UNIX/POSIX. The microbenchmarks are designed to provide the latency and bandwidth deatils of diferent system operations. The microbenchmarks are extensible in nature and target specific subsytems and operations to measure hardware performances for example fork benchmark targets the performance anlaysis of process creation only.

{\tt Unixbench} is also a benchmarking suite to analyze the system performances of Unix-like systems. It also contains the microbenchmarks to test various aspects of system performance. These micro-benchmarks are also specific in nature and targets different subsytems of the system. A major difference with the lmbench benchmarking suite is that while lmbench suite provides the latency of the different subsystems it can't be used to compare the tlb misses, privileged exits and tracing page faults since the number of executions of the micro-benchmark depends on the latency of the benchmark itself. The Unixbench microbenchmarks can be tunes to execute the benchmarks for a fixed number of times giving more coorect data about the overheads and performance optimizations due to adaptive binary translation and tracing.

The number of privileged instructions executed by each benchmark are reported in Table~\ref{tab:vm_exit_stats}. We do not report performance comparisons on purely user-level compute-intensive workloads because they exhibit near bare-metal performance for all cases.

We implement the following optimizations in Linux/KVM:
\begin{enumerate}
  \item In-place Binary Translation and Read/Write Tracing
  \item Adaptive Page Resizing
  \item Adaptive Data Mirroring and Translation of Faulting Instructions
\end{enumerate}
Table~\ref{tab:detailed_results} summarizes our performance results before and after these optimizations. Different workloads show different improvements. The improvement primarily depends on a three-way tradeoff between the number of VM exits due to privileged instructions, the number of tracing page faults, and the number of page faults due to increased TLB pressure (TLB misses). Table~\ref{tab:vm_exit_stats} shows the number of VM exits and page faults due to all these three reasons for each workload, before and after our optimizations. We also show the number of exits in the paravirtual solution ({\tt KVM-PV}) for comparison. The number of exits in {\tt KVM-PV} can be considered as a lower-bound on the number of exits achievable by a host-side binary translation solution.

Just using Optimization 1 does not improve performance for a Linux guest. In fact, we find that read/write tracing severely impairs performance because the guest uses a huge 256MB page to map the kernel's code and data. If we constrain the host to only use 4KB shadow pages, the performance improves but is still unacceptably low due to high TLB pressure.

\begin{table*}
\centering
      \begin{tabular}{|l| r r r |} \hline
        Benchmark\verb, ,& {\tt KVM} \verb, , & {\tt Adapt-PR} \verb, , & {\tt Adapt-DM}  \\ \hline
     & \multicolumn{3}{c|}{ Running Time in $sec$}\\ \cline {2-4}  
Linux boot	&	30.03	&	13.02	&	12.39	\\
Echo Spawn	&	21.34	&	7.80	&	6.85	\\
Find	&	1.89	&	0.98	&	0.83	\\
Lame	&	0.56	&	0.51	&	0.50	\\
\cline{2-4}
	   
     & \multicolumn{3}{c|}{Latency in $msec$}\\  \cline{2-4}
syscall	&	0.020	&	0.009	&	0.003	\\
stat	&	0.033	&	0.012	&	0.007	\\
fstat	&	0.021	&	0.009	&	0.004	\\
open/close:	&	0.067	&	0.026	&	0.023	\\
sig hndl	&	0.024	&	0.009	&	0.004	\\
pipe 	&	0.066	&	0.062	&	0.041	\\
fork	&	6.641	&	2.020	&	1.679	\\
exec	&	20.543	&	7.457	&	6.681	\\
sh	&	45.164	&	16.567	&	14.719	\\
 \hline
\cline{1-4}
\multicolumn{1}{|c|}{Unixbench Benchmark}     & \multicolumn{3}{c|}{Number of Calls}\\  \cline{1-4}
Dhrystone 2	&	48110141	&	48904947	&	48957180\\
System Call	&	124854	&	286057	&	652829\\
Pipe-based context switching	&	60746	&	159256	&	240653\\
Process Creation	&	2470	&	7679	&	8714\\
Pipe Throughput	&	188208	&	469538	&	923218\\
Hanoi	&	8689401	&	8690019	&	8836219\\
\cline{1-4}
\multicolumn{1}{|c|}{Unixbench Filesystem Benchmark}     & \multicolumn{3}{c|}{Throughput in KBps with 256 bufsize and 2000 max blocks
}\\  \cline{1-4}
File Read	&	11524	&	24151	&	49830\\
File Write	&	10500	&	19050	&	38200\\
File Copy	&	5432	&	10408	&	20070\\	\cline{1-4}

      \end{tabular}
\caption{\label{tab:detailed_results}Performance Improvements obtained by Adaptive Page Resizing ({\tt Adapt-PR}) and Adaptive Data Mirroring ({\tt Adapt-DM}) optimizations.}
\end{table*} 

\begin{center}
\begin{table*}
\centering
\begin{tabular}{
  |p{1.2cm}@{}|@{ }c@{ }|@{}p{1.3cm}|@{}p{1.0cm}|@{}p{1.0cm}@{}|@{ }c@{ }|@{}p{1.2cm}|@{}p{1.2cm}|@{}p{1.0cm}|@{ }c@{ }|@{}p{1.2cm}|@{}p{0.9cm}|@{}p{0.9cm}|@{ }c@{ }|@{}p{1.2cm}|@{}p{0.9cm}|@{}p{0.9cm}@{}|} \hline
	         &  & \multicolumn{3}{c|}{\tt KVM}& & \multicolumn{3}{c|}{\tt Adapt-PR} & & \multicolumn{3}{c|}{\tt Adapt-DM} & & \multicolumn{3}{c|}{\tt KVM-PV} \\ \hline

           &&{\tt Priv. Exits}&{\tt TLB miss}&{\tt Acc. Viol} &&{\tt Priv. Exits}&{\tt TLB miss}&{\tt Acc. Viol.}&&{\tt Priv. Exits}&{\tt TLB miss}&{\tt Acc. Viol}&&{\tt Priv. Exits}&{\tt TLB miss}&{\tt Acc. Viol} \\ \hline  
Boot	&&	11509859	&	402274	&	13823	&&	48560	&	326825	&	460001	&&	38367	&	345035	&	77467	&&	40451	&	295672	&	14181	\\	\hline
Find	&&	802777	&	2485	&	46	&&	965	&	2655	&	98145	&&	851	&	2782	&	4701	&&	719	&	2392	&	46	\\	\hline
Lame	&&	60312	&	1932	&	23	&&	636	&	2085	&	3126	&&	644	&	2114	&	46	&&	625	&	1916	&	24	\\	\hline
Echo Spawn	&&	9829128	&	419191	&	18005	&&	35823	&	492129	&	518255	&&	35014	&	506295	&	18624	&&	34602	&	427351	&	17011	\\	\hline
Pipe 	&&	28714485	&	979	&	41	&&	18762	&	1113	&	6010468	&&	9185	&	1165	&	75	&&	7648	&	968	&	42	\\	\hline
Hanoi	&&	4005414	&	914	&	40	&&	225211	&	1088	&	151308	&&	224970	&	1169	&	876	&&	224660	&	943	&	41	\\	\hline
Syscall	&&	22541901	&	953	&	41	&&	15105	&	1130	&	6008149	&&	6324	&	1159	&	40	&&	5197	&	948	&	41	\\	\hline
Process Creation	&&	19841882	&	621658	&	60036	&&	100796	&	835408	&	897302	&&	99081	&	846053	&	60026	&&	98569	&	652042	&	60034	\\	\hline
Context switching	&&	26459239	&	1102	&	49	&&	616684	&	7702	&	3609690	&&	610986	&	5567	&	49	&&	608549	&	1059	&	49	\\	\hline
Dhrystone 2	&&	154460	&	973	&	40	&&	7656	&	1127	&	6132	&&	7648	&	1140	&	38	&&	7667	&	1001	&	40	\\	\hline
      \end{tabular}
\caption{\label{tab:vm_exit_stats}Frequency of VM exits due to privileged instructions ({\tt Priv. Exits}), TLB misses ({\tt TLB miss}), and access violations ({\tt Acc. Viol}). The
exits due to access violations are due to read/write accesses to pages which do not have read/write permissions. These exits occur primarily due to read/write tracing.}
\end{table*} 
\end{center}

Optimization 2 adaptively resizes the shadow TLB entries to reduce the number of tracing page faults and results in performance improvements over unmodified KVM. The second column ({\tt Adapt-PR}) in Table~\ref{tab:detailed_results} shows the performance of KVM with Optimizations~1~and~2. We first discuss the {\tt lmbench} microbenchmarks. The running time of all microbenchmarks improves with Optimizations~1~and~2. The average improvement is 148\%, with the maximum improvement of 223\% in fork. The improvement in a benchmark primarily depends on the reduction in the number of privileged exits. Different microbenchmarks execute different number of privileged instructions and show different corresponding reductions. We next discuss the improvement on macrobenchmarks using Optimizations~1~and~2. We observe a performance improvement of 101\% on the macrobenchmarks, with the maximum improvement seen in Echo Spawn, which also shows the maximum reduction (99\%) in exits due to privileged instructions. For both microbenchmarks and macrobenchmarks, we notice an increase in the number of TLB misses and access violations (due to tracing page faults) on average. The net effect however remains positive. Our next optimization (Optimization~3) further reduces these access violations. We also find that Optimization~3 reduces privileged instruction exits. A slight increase in tlb misses can be observed after Optimization~3 this is due to the fact that in the solution obtained from Optimization~1~and~2  the translation cache is being allocated by the guest while Optimization~3 steals it from the data section causing a slight increase in the tlb misses.

Optimization~3 reduces the number of tracing page faults by mirroring the data residing on a traced page and patching the corresponding access instruction to access the data from the mirrored location in future. The reduction is observed in all our benchmarks. The average reduction in the number of access-violation page faults (which are mostly due to tracing page faults) is XX\%.
