\chapter{Introduction}\label{ch:1}
Embedded devices based on Power Architecture processors are dominant for their favourable power/performance characteristics. Virtualization on these platforms is compelling for several applications including high availability (active/standby configuration without additional hardware), in-service upgrade, and many more\cite{embedded_virtualization, KVM_on_embedded_Power}. While newer Power Architecture platforms have explicit support for efficient virtualization\cite{freescale_embedded_hyperv, hwassists_hyperv}, a majority of prevalent embedded devices run on older (and cheaper) Power Architecture platforms that use traditional trap-and-emulate style virtualization\cite{popekgoldberg}. Efficient virtualization is highly desirable on these platforms.

The current virtualization approach on Power Architecture platforms uses traditional trap-and-emulate. The guest operating system is run unprivileged, causing each execution of a privileged operation to exit into the hypervisor. For guest workloads executing a large number of privileged instructions, these VM exits are a major performance bottleneck. Table~\ref{tab:kvm_performance} lists the performance of vanilla Linux/KVM on a few common workloads, comparing them with bare-metal performance. For example, a guest Linux boot takes almost 5x longer when run virtualized.

The poor performance of simple trap-and-emulate style virtualization has led to the inclusion of paravirtual extensions in the Linux kernel on both guest and host sides for Power Architecture platform\cite{pvpower}. The paravirtual extension in the guest rewrites the guest (binary) kernel code at startup time to replace most privileged instructions with hypervisor-aware unprivileged counterparts. At guest startup, the guest creates a shared address space with the host through a hypercall. This shared address space is used by the hypervisor to store guest state information, which is accessible to the guest without incurring a trap. Table~\ref{tab:kvm_performance} lists KVM performance after enabling paravirtual extensions in the guest and the host. The performance improves significantly over unmodified KVM.

The paravirtual approach has shortcomings. Firstly, extensive guest modifications are required, which makes the optimizations highly Linux specific. Secondly, this approach cannot optimize dynamically generated/loaded code (e.g., loadable modules) because all translation is done at kernel startup time. The paravirtual approach also fails in presence of self-referential and self-modifying code in guest, as binary rewriting is done only once at startup time, and other parts of guest kernel may be unaware of this rewriting operation. The Linux guest paravirtual extensions rely on modified code not being referred or modified. These constraints are ungraceful, and hard to maintain.

We propose a host-side adaptive binary translation mechanism to optimize guest privileged instructions at runtime. Our approach improves performance for an unmodified, untrusted guest and is more general than the paravirtualization approach; we can optimize dynamically generated/loaded code, and can gracefully handle self- referential and self-modifying code in the guest. We still achieve comparable performance to the paravirtual approach. The second-last column in Table~\ref{tab:kvm_performance} summarizes the performance results of our host-side binary translation approach. We perform our experiments on Linux/KVM running on Freescale P2020 embedded Power Architecture platform.

Our host-side virtualization optimizations are based on adaptive binary translation. On observing a large number of VM exits by a guest instruction, we translate that instruction in-place to directly execute the corresponding VMM logic (thus avoiding an exit). In doing so, we directly modify the guest’s address space. This is in contrast to a full binary translation approach that translates the entire guest code (e.g., VMware’s x86-based binary translator\cite{adams:asplos06}). Our approach is simpler and incurs less overhead than full binary translation approaches.

A major challenge with inplace guest modification is dealing with self-referential and self-modifying guest code. Since the guest address space has been modified without the guest's knowledge, the guest may access the modified regions resulting in unexpected behavior. For example, it is common for an OS to perform runtime integrity checks on its code/data. Such checks can fail due to our {\em hidden} modifications.
We maintain correctness by marking pages containing the modified instructions execute-only, using hardware page protection bits. This mechanism is
called {\em read-write tracing}, and the protected page is called
a {\em traced page}. Any guest access to a traced page
traps (page fault) and causes an exit to the VMM. A guest exit is then
handled in VMM by correctly emulating the guest access in software.
Hence, the guest behaviour after optimizations always remains correct.

Read/write tracing can result in a large number of page faults.
Because hardware page protection bits work at page granularity, accesses
to unpatched regions belonging to the same page also trap.
The problem is exacerbated on embedded Power Architecture platform, where OS
typically uses huge pages to reduce TLB pressure. We found that such page
faults can significantly reduce performance. We implement two important
optimizations to address this problem, namely {\em adaptive page resizing}
and {\em adaptive data mirroring}.

In summary, this project presents an efficient host-side optimization solution for Power Architecture virtualization. Our approach significantly improves the performance of an unmodified guest. The scheme is based on in-place binary translation, and maintains guest correctness in presence of self-referential and self-modifying guest code.

%\begin{table*}

%\centering
\newpage
%      \noindent \makebox[\textwidth]{%
      \begin{longtable}{|l@{}| p{1.8cm} |p{3.0cm} | p{1.2cm}| p{1.2cm}| p{1.2cm}| p{1.2cm}| p{1.0cm}|}
	\caption{\label{tab:kvm_performance}Performance comparison of bare-metal, unmodified KVM, KVM-paravirtual, and our (KVM-BT) approach. The details of the benchmarks, our test system, and the various KVM variants is discussed in Section~\ref{ch:6}. The last column computes the speedup of {\tt KVM-BT} over {\tt KVM}.} \\ \hline
        S.No.\verb, ,&  Benchmark\verb, ,& Description  & Bare-metal \verb, ,& {\tt KVM} \verb, , & {\tt KVM PV} \verb, ,& {\tt KVM BT}& Speedup \\ \hline
	\endfirsthead

	\multicolumn{8}{c}%
	{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
	\hline \multicolumn{1}{|c|}{S.No.} &
	\multicolumn{1}{c|}{Benchmark} &
	\multicolumn{1}{c|}{Description} &
	\multicolumn{1}{c}{Bare-metal} &
	\multicolumn{1}{c}{{\tt KVM}} &
	\multicolumn{1}{c}{{\tt KVM PV}} &
	\multicolumn{1}{c}{{\tt KVM BT}} &
	\multicolumn{1}{c|}{Speedup} \\ \hline
	\endhead

	\hline \multicolumn{8}{|r|}{{Continued on next page}} \\ \hline
	\endfoot

	\hline \hline
	\endlastfoot

     &&& \multicolumn{5}{c|}{ Running Time in $sec$}\\\cline {4-8}  
      1&  Linux boot& Boots a Linux 3.0 guest & 6.5	&	30.03	&	11.79	&	12.39 & 2.4x \\ \hline
      2& Echo spawn	& Spawns 1000 echo processes &1.4	&	21.34	&	6.5	&	6.85& 3.1x \\\hline
      3& Find	& Executes {\tt 'find / -name temp'} & 0.39	&	1.89	&	0.67	&	0.83 & 2.3x\\ \hline
      4& Lame	& MP3 encoder & 0.44	&	0.56	&	0.49	&	0.50 & 1.1x\\ \hline
	   \multicolumn{3}{|c|}{ lmbench microbenchmarks }& \multicolumn{5}{c|}{Latency in $msec$}\\  \hline

5	&	syscall	&	 Writes one word to /dev/null	&	0.0002	&	0.020	&	0.003	&	0.003 & 6.7x	\\	\hline
6	&	stat	&	 Invokes the stat system call	&	0.003	&	0.033	&	0.006	&	0.007& 4.7x	\\	\hline
7	&	fstat	&	Invokes fstat system call on an open file 	&	0.001	&	0.021	&	0.004	&	0.004&5.3x	\\	\hline
8	&	open/close:	&	Opens a temporary file for reading and closes it immediately 	&	0.006	&	0.067	&	0.013	&	0.023&2.9x	\\	\hline
9	&	sig hndl	&	Installs a signal handler	&	0.001	&	0.024	&	0.004	&	0.004	& 6x\\	\hline
10	&	pipe 	&	Passes a word from process A to process B and back to A	and measures round-trip time &	0.003	&	0.066	&	0.010	&	0.041&1.6x	\\	\hline
11	&	fork	&  Calls {\tt fork} and {\tt exit} 	&	1.084	&	6.641	&	1.640	&	1.679&3.9x	\\	\hline
12	&	exec	& Calls {\tt fork}, {\tt exec} and {\tt exit} &	3.065	&	20.543	&	6.254	&	6.681&3.1x	\\	\hline
13	&	sh	& Calls {\tt fork}, {\tt exec sh -c} and {\tt exit}	&	6.645	&	45.164	&	13.842	&	14.719&3.1x	\\	\hline	
	\multicolumn{3}{|c|}{ Unixbench microbenchmarks }& \multicolumn{5}{c|}{Number of Calls in 10s }\\  \hline
14	&	Dhrystone 2	& Focuses on string handling	&	49697211	&	48110141	&	49014236	&	48957180	&	1.02x	\\	\hline
15	&	System Call	& Calls the getpid system call	&	7863359	&	124854	&	818940	&	652829	&	5.2x	\\	\hline
16	&	Pipe-based Context Switching	& Spawns a child process with which it carries on a bi-directional pipe conversation	&	420968	&	60746	&	307136	&	240653	&	4.0x	\\	\hline
17	&	Process Creation	& Forks and reap a child that immediately exits.	&	44667	&	2470	&	9432	&	8714	&	3.5x	\\	\hline
18	&	Pipe Throughput	& Writes 512 bytes to a pipe and read them back	&	3324145	&	188208	&	1111797		&	923218	&	4.9x	\\	\hline
19	&	Hanoi	& Calls Hanoi Program	&	8853023	&	8689401	&	8846663	&	8836219	&	1.02x	\\	\hline
	\multicolumn{3}{|c|}{ Unixbench Filesystem microbenchmarks }& \multicolumn{5}{c|}{Values in KBps with 256 bufsize and 2000 max blocks }\\  \hline
20	&	File Read 	& Read from a file	&	182340	&	11524 	&	58647	&	49830	&	4.3x	\\	\hline
21	&	File Write 	& Write on a file	&	99850	&	10500 	&	39500	&	38200	&	3.6x	\\	\hline
22	&	File Copy 	& Transfers data from one file to another	&	59612	&	5432 	&	22225	&	20070	&	3.7x	\\	\hline
        \hline

      \end{longtable}
%	}
%\end{table*} 

