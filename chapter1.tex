\chapter{Introduction}\label{ch:1}
Embedded devices based on Power Architecture processors are dominant for their favourable power/performance characteristics. Virtualization on these platforms is compelling for several applications including high availability (active/standby configuration without additional hardware), in-service upgrade, and many more\cite{embedded_virtualization, KVM_on_embedded_Power}. While newer Power Architecture platforms have explicit support for efficient virtualization\cite{freescale_embedded_hyperv, hwassists_hyperv}, a majority of prevalent embedded devices run on older (and cheaper) Power Architecture platforms that use traditional trap-and-emulate style virtualization\cite{popekgoldberg}. Efficient virtualization is highly desirable on these platforms.

The current virtualization approach on Power Architecture platforms uses traditional trap-and-emulate. The guest operating system is run unprivileged, causing each execution of a privileged operation to exit into the hypervisor. For guest workloads executing a large number of privileged instructions, these VM exits are a major performance bottleneck. Table~\ref{tab:kvm_performance} lists the performance of vanilla Linux/KVM on a few common workloads, comparing them with bare-metal performance. For example, a guest Linux boot takes almost 5x longer when run virtualized.

The poor performance of simple trap-and-emulate style virtualization has led to the inclusion of paravirtual extensions in the Linux kernel on both guest and host sides for Power Architecture platform\cite{pvpower}. The paravirtual extension in the guest rewrites the guest (binary) kernel code at startup time to replace most privileged instructions with hypervisor-aware unprivileged counterparts. At guest startup, the guest creates a shared address space with the host through a hypercall. This shared address space is used by the hypervisor to store guest state information, which is accessible to the guest without incurring a trap. Table~\ref{tab:kvm_performance} lists KVM performance after enabling paravirtual extensions in the guest and the host. The performance improves significantly over unmodified KVM.

The paravirtual approach has shortcomings. Firstly, extensive guest modifications are required, which makes the optimizations highly Linux specific. Secondly, this approach cannot optimize dynamically generated/loaded code (e.g., loadable modules) because all translation is done at kernel startup time. The paravirtual approach also fails in presence of self-referential and self-modifying code in guest, as binary rewriting is done only once at startup time, and other parts of guest kernel may be unaware of this rewriting operation. The Linux guest paravirtual extensions rely on modified code not being referred or modified. These constraints are ungraceful, and hard to maintain.

We propose a host-side adaptive binary translation mechanism to optimize guest privileged instructions at runtime. Our approach is more general than the paravirtualization approach; we can optimize dynamically generated/loaded code, and can gracefully handle self- referential and self-modifying code in the guest and still achieve comparable performance to the paravirtual approach. The second-last column in Table~\ref{tab:kvm_performance} summarizes the performance results of our host-side binary translation approach. We perform our experiments on Linux/KVM running on Freescale P2020 embedded Power Architecture platform.

Our host-side virtualization optimizations are based on adaptive binary translation. On observing a large number of VM exits by a guest instruction, we translate that instruction in-place to directly execute the corresponding VMM logic (thus avoiding an exit). In doing so, we directly modify the guest’s address space. This is in contrast to a full binary translation approach that translates the entire guest code (e.g., VMware’s x86-based binary translator\cite{adams:asplos06}). Our approach is simpler and incurs less overhead than full binary translation approaches.

A major challenge with inplace guest modification is dealing with self-referential and self-modifying guest code. Since the guest address space has been modified without the guest's knowledge, the guest may refer them resulting in an undefined behavior. Also a security-conscious guest can get confused due to these changes. For example, what if the guest is performing runtime integrity checks on its code/data? Also these optimizations requires a shared space between the hypervisor and the guest to implement the VMM logic both in paravirtual extensions as well as host side optimizations. Can a malicious guest kernel/module pose a VMM security threat using this shared space ? Can we guarantee the guest's fidelity with these optimizations ?

We maintain correctness by marking pages containing the modified instructions execute-only.. The page thus protected is called a “traced page”. This causes the hardware to trap into the VMM on any guest read/write access to these traced pages. All these exits are emulated inside the hypervisor to ensure the correctness of the guest. We call this mechanism read/write tracing. This mechanisms ensures the guest's fidelity to be the same as before the optimizations.

Finally, read/write tracing can result in a large number of page faults(VM exits). In essence read/tracing intends to protect all accesses by guest on modified codes but due to constraint of page granularities, this leads in false regions being traced. The problem is exacerbated on embedded Power Architecture platform, where OS typically uses huge pages to reduce TLB pressure. We found that such page faults can significantly reduce performance. We implement two important optimizations to address this problem, namely adaptive page resizing and adaptive data mirroring.

In summary, this project presents an efficient host-side optimization solution for Power Architecture virtualization. Our approach significantly improves the performance of an unmodified guest. The scheme is based on in-place binary translation, and maintains guest correctness in presence of self-referential and self-modifying guest code.

%\begin{table*}

%\centering
\newpage
%      \noindent \makebox[\textwidth]{%
      \begin{longtable}{|l@{}| p{1.8cm} |p{3.0cm} | p{1.2cm}| p{1.2cm}| p{1.2cm}| p{1.2cm}| p{1.0cm}|}
	\caption{\label{tab:kvm_performance}Performance comparison of bare-metal, unmodified KVM, KVM-paravirtual, and our (KVM-BT) approach. The details of the benchmarks, our test system, and the various KVM variants is discussed in Section~\ref{ch:6}. The last column computes the speedup of {\tt KVM-BT} over {\tt KVM}.} \\ \hline
        S.No.\verb, ,&  Benchmark\verb, ,& Description  & Bare-metal \verb, ,& {\tt KVM} \verb, , & {\tt KVM PV} \verb, ,& {\tt KVM BT}& Speedup \\ \hline
	\endfirsthead

	\multicolumn{8}{c}%
	{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
	\hline \multicolumn{1}{|c|}{S.No.} &
	\multicolumn{1}{c|}{Benchmark} &
	\multicolumn{1}{c|}{Description} &
	\multicolumn{1}{c}{Bare-metal} &
	\multicolumn{1}{c}{{\tt KVM}} &
	\multicolumn{1}{c}{{\tt KVM PV}} &
	\multicolumn{1}{c}{{\tt KVM BT}} &
	\multicolumn{1}{c|}{Speedup} \\ \hline
	\endhead

	\hline \multicolumn{8}{|r|}{{Continued on next page}} \\ \hline
	\endfoot

	\hline \hline
	\endlastfoot

     &&& \multicolumn{5}{c|}{ Running Time in $sec$}\\\cline {4-8}  
      1&  Linux boot& Boots a Linux 3.0 guest & 6.5	&	30.03	&	11.79	&	12.39 & 2.4x \\ \hline
      2& Echo spawn	& Spawns 1000 echo processes &1.4	&	21.34	&	6.5	&	6.85& 3.1x \\\hline
      3& Find	& Executes {\tt 'find / -name temp'} & 0.39	&	1.89	&	0.67	&	0.83 & 2.3x\\ \hline
      4& Lame	& MP3 encoder & 0.44	&	0.56	&	0.49	&	0.50 & 1.1x\\ \hline
	   \multicolumn{3}{|c|}{ lmbench microbenchmarks }& \multicolumn{5}{c|}{Latency in $msec$}\\  \hline

5	&	syscall	&	 Writes one word to /dev/null	&	0.0002	&	0.020	&	0.003	&	0.003 & 6.7x	\\	\hline
6	&	stat	&	 Invokes the stat system call	&	0.003	&	0.033	&	0.006	&	0.007& 4.7x	\\	\hline
7	&	fstat	&	Invokes fstat system call on an open file 	&	0.001	&	0.021	&	0.004	&	0.004&5.3x	\\	\hline
8	&	open/close:	&	Opens a temporary file for reading and closes it immediately 	&	0.006	&	0.067	&	0.013	&	0.023&2.9x	\\	\hline
9	&	sig hndl	&	Installs a signal handler	&	0.001	&	0.024	&	0.004	&	0.004	& 6x\\	\hline
10	&	pipe 	&	Passes a word from process A to process B and back to A	and measures round-trip time &	0.003	&	0.066	&	0.010	&	0.041&1.6x	\\	\hline
11	&	fork	&  Calls {\tt fork} and {\tt exit} 	&	1.084	&	6.641	&	1.640	&	1.679&3.9x	\\	\hline
12	&	exec	& Calls {\tt fork}, {\tt exec} and {\tt exit} &	3.065	&	20.543	&	6.254	&	6.681&3.1x	\\	\hline
13	&	sh	& Calls {\tt fork}, {\tt exec sh -c} and {\tt exit}	&	6.645	&	45.164	&	13.842	&	14.719&3.1x	\\	\hline	
	\multicolumn{3}{|c|}{ Unixbench microbenchmarks }& \multicolumn{5}{c|}{Number of Calls in 10s }\\  \hline
14	&	Dhrystone 2	& Focuses on string handling	&	49697211	&	48110141	&	49014236	&	48957180	&	1.02x	\\	\hline
15	&	System Call	& Calls the getpid system call	&	7863359	&	124854	&	818940	&	652829	&	5.2x	\\	\hline
16	&	Pipe-based Context Switching	& Spawns a child process with which it carries on a bi-directional pipe conversation	&	420968	&	60746	&	307136	&	240653	&	4.0x	\\	\hline
17	&	Process Creation	& Forks and reap a child that immediately exits.	&	44667	&	2470	&	9432	&	8714	&	3.5x	\\	\hline
18	&	Pipe Throughput	& Writes 512 bytes to a pipe and read them back	&	3324145	&	188208	&	1111797		&	923218	&	4.9x	\\	\hline
19	&	Hanoi	& Calls Hanoi Program	&	8853023	&	8689401	&	8846663	&	8836219	&	1.02x	\\	\hline
	\multicolumn{3}{|c|}{ Unixbench Filesystem microbenchmarks }& \multicolumn{5}{c|}{Values in KBps with 256 bufsize and 2000 max blocks }\\  \hline
20	&	File Read 	& Read from a file	&	182340	&	11524 	&	58647	&	49830	&	4.3x	\\	\hline
21	&	File Write 	& Write on a file	&	99850	&	10500 	&	39500	&	38200	&	3.6x	\\	\hline
22	&	File Copy 	& Transfers data from one file to another	&	59612	&	5432 	&	22225	&	20070	&	3.7x	\\	\hline
        \hline

      \end{longtable}
%	}
%\end{table*} 

